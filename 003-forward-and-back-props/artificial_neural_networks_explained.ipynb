{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When dealing with a supervised learning problem on a large scale, sometimes using a linear regression model will not do the trick. There are some problems that don't fit such a model -- such as a classification problem. These problems, instead of seeking to draw a line between points, want to predict which class an entity will fall into given some of its characteristics. An example would be predicting if a student will be admitted into a univerity based off of his/her grades or class rank, for instance. Several techniques exist to solve such problems, such as logistic and softmax regression. In this tutorial, I'll introduce the current state-of-the-art: Artificial Neural Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of artificial neural networks is to use a mathematical abstraction of the human brain to solve supervised learning problems. To accomplish this, we create a series of artificial neurons, and string them together to form a network. By activating or tuning the network, we can solve many tasks, such as regression and classification problems. The main strength of neural nets is that, by using a neural network, we can predict non-linear hypotheses! Consequently, in theory, our neural network can learn to map any function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The critical idea here is to simulate the biological neuron:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"neuron.png\" alt=\"A biological neuron\" title=\"Neuron\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(image taken from Wikipedia.org)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A biological neuron works by taking inputs, or feedback from other neurons, through its dendrites. Some kind of computation is performed in the neuron's axons, which is then transmitted out of the synaptic terminals to act as input to other neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a vast simplification of how a biological neuron actually works, however, it is sufficient for us to create a mathematical model that we can execute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"artificial_neuron.png\" alt=\"An artificial neuron.\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is essentially the abstraction we use when creating an artificial neural network. X1, X2, and X3 are all numerical inputs. These could be features of a house in a price-predicting problem, for example. f(X) is a mathematical function, which takes each of the inputs and computes an output. Our output is denoted y, and is the result of passing our X values into f(x)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We associate three weights, W1, W2, and W3, with each of the X inputs. f(x) multiplies each of the X values by its respective W value, and adds the summation together. The result of this summation is y.\n",
    "\n",
    "So W1X1+W2X2+W3X3 is the result of f(x), and that result is given as our output, y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our weights, when combined with the inputs, should be fine-tuned to produce a meaningful y output. You may have observed that this model is similar to linear regression, in how coefficients are combined with inputs to produce a y value. An artificial neuron uses this idea, but by stringing many artificial neurons together, we can predict more complex hypotheses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Artificial neurons can be strung together into what's called a layer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"1layer.png\" alt=\"A layer of artificial neurons.\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(image taken from researchgate.net, Eric Gaume)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each input, from the input layer, is associated with a weight in each neuron. Each layer of neurons is called a hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every neuron in the hidden layer will take a linear combination of all of the inputs and its weights, which are unique for each neuron. The results from all of these neurons can be used as inputs for an output neuron. The output neuron has its own set of weights, and takes a linear combination of those weights with the outputs from the hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of considering all of the inputs as individual entities, and iterating through them, it would be better notationally to consider all of the inputs (X1 through Xn) as a vector, which we will write as X. We can similarly write the weights as a vector, W."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "When the hidden layer computes the linear combinations of the weights and its inputs, it creates a number that is typically denoted Z. We can perform some computations on Z to create what's called the activation function, or A. We also denote Z and A as vectors, similarly to what we did for X and W. Consequently, with our vectorized implementations, we compute Z in a vectorized manner. By taking Z=(W<sup>T</sup>X), we can fully vectorize the outputs of our artificial neural network without iterating through each weight and input. This is important when coding an artificial neural network, as a computer can make use of hardware level linear algebra routines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more sophisticated neural network can have several hidden layers, and use the activations of the previous layer as inputs. The activations of the previous layer are linearly combined with the weights of the current layer, just like we did when we used X as inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ann.jpg\" alt=\"An artificial neural network with multiple layers.\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(image taken from medium.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that each layer will have its own weights, as well as its own linear combinations and activations. We denote these, for each layer L, W<sup>L</sup>, Z<sup>L</sup>, and A<sup>L</sup>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another key idea in creating an artificial neural network is the introduction of a bias unit. By adding each linear combination by some constant, which we tune along with the weights, we can get more accurate data. Each of these bias values b<sup>L</sup> corresponds to a layer L, and all of the bias units can be formed into a vector b."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, each A<sup>L</sup> is given by A<sup>L-1</sup>W<sup>L</sup>+b<sup>L</sup>, where A<sup>0</sup> is X. However, I made the claim earlier that a neural network can represent a non linear hypothesis. To create a non linear hypothesis, we need to introduce another function to our neural network that will change our activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way of accomplishing this is to put every linear combination in Z<sup>L</sup> through what's called the sigmoid function. This will result in A<sup>L</sup> always being between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sigmoid function is given by ${1}/(1+e^{-z})$ where z is the value given by our linear combination. By passing all of our linear combinations through a function such as the sigmoid function, we can predict non linear hypotheses. Other candidates for the nonlinearity include the relu function as well as the tanh function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward propagation is a procedure used to determine the output of a neural network. It is given by "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for each layer L:\n",
    "A<sup>L</sup> = g(A<sup>L-1</sup>W<sup>L</sup>+b<sup>L</sup>) where g is a nonlinearity such as sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of g(Z) on the last layer will be our output. This output is just a prediction, which could be the value of a house in a regression problem or whether or not the input variables belong to a class in a classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be noted that the programmer may choose different g(Z) functions at each layer, or none at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Backward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our artificial neural network is useless if it can't have its weights tuned. The method of tuning weights in a neural network is called backward propagation. This is because we start at the end of the network, take how different our predictions are from the actual y value of our supervised training example, and use that to determine how much the weights and biases in our network should be changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is done using what's called a derivative. If you haven't studied calculus, the derivative is essentially the rate of change of a variable. The derivative of a variable x is denoted dx. So dx is the rate at which x changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using backward propagation (backprop for short) we compute the derivatives of the linear combination Z, the weight vector W, the bias b, and the activation vector A at each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by computing the last dz, which is the difference between our prediction and the actual value of a training example. So dz=y-$\\hat{y}$ where $\\hat{y}$ is the value our artificial neural network predicts for the training example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each layer L, we compute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dz<sup>L</sup> = dA<sup>L</sup>g<sup>L</sup>'(Z<sup>L</sup>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dw<sup>L</sup> = 1/m * dz<sup>L</sup> A<sup>L-1<sup>T</sup></sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "db<sup>L</sup> = 1/m * dz<sup>L</sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dA<sup>L-1</sup> = W<sup>L<sup>T</sup></sup>dz<sup>L</sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where g<sup>L</sup>' is the derivative of our nonlinearity function at layer L, and A<sup>L-1<sup>T</sup></sup> is the transposition of A<sup>L-1</sup>. Keep in mind, when we conduct this algorithm, we start at the final layer, and compute dA<sup>L-1</sup> for the preceding layer. We keep this going, and eventually move all the way back to the starting layer. This is why we let the first dz be the difference between y and our prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should also be noted that to implement this practically, the value of A at each layer during forward propagation should be saved for when backprop is conducted. In order for A<sup>L-1</sup> to be referenced in backprop, we need to store each activation function during forward prop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have calculated all of the derivatives, we need to update our weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W<sup>L</sup> = W<sup>L</sup> - $\\alpha$ * dw<sup>L</sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b<sup>L</sup> = b<sup>L</sup> - $\\alpha$ * db<sup>L</sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By performing backpropagation enough times, we will find weights that suit the problem the artificial neural network seeks to solve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should also be noted that the derivative of the nonlinearity should also be kept on hand. The derivative of g(z) where g(z) is the sigmoid function is g(z)(1-g(z)), or a(1-a). The derivative of g(z) where g(z) is the tanh function is $1-g(z)^2$, or $1-a^2$. The derivative of ReLU is $1/(1+e^{-z})$ when z<0, and $-(1+e^{-z})^{-2}$ * $-e^{-z}$ when Z is greater than or equal to 0. If we are working with an artificial neural network framework such as Theano or Tensorflow, this is handled automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing to note is that the W<sup>L</sup> formulas are using $\\alpha$, which is the learning rate parameter. Ideally we want this to be between 0 and 1, and typically less than 0.1. This controls how fast the weights and bias units are optimized. If we set $\\alpha$ to be too high, the weights and bias units will be updated dramatically at each iteration of gradient descent. This may become problematic, as it will cause the learning carried out by the artificial neural network to be very unstable. If we set $\\alpha$ to be too low, it won't find relevant weights in a reasonable amount of time. We can find a good value for $\\alpha$ by trying out different values of $\\alpha$ and seeing which ones converge upon optimal weight values the fastest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should also be considered that these weight update rules use gradient descent. Various other optimizations of gradient descent can give your neural network faster learning, such as RMSprop, gradient descent with momentum, and Adam. I would highly recommend investigating these, as most modern programs use that kind of an update step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This just about wraps it up for forward propagation and backward propagation, I urge you to try it out for yourself with numpy and see what works best for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
