{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./inputs/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./inputs/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./inputs/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./inputs/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('./inputs/mnist')\n",
    "# Resetting default graph, starting from scratch\n",
    "tf.reset_default_graph()\n",
    "\n",
    "batch_size = 64\n",
    "n_noise = 200\n",
    "learning_rate=0.00015\n",
    "\n",
    "X_in = tf.placeholder(dtype=tf.float32, shape=[None, 28, 28, 1], name='X')\n",
    "noise = tf.placeholder(dtype=tf.float32, shape=[None, n_noise])\n",
    "\n",
    "# The keep_prob variable will be used by our dropout layers, which we introduce for more stable learning outcome\n",
    "keep_prob = tf.placeholder(dtype=tf.float32, name='keep_prob')\n",
    "is_training = tf.placeholder(dtype=tf.bool, name='is_training')\n",
    "\n",
    "# Leaky Relu activation\n",
    "# https://en.wikipedia.org/wiki/Rectifier_%28neural_networks%29#Potential_problems\n",
    "def lrelu(x):\n",
    "    return tf.maximum(x, tf.multiply(x, 0.2))\n",
    "\n",
    "# Binary cross entropy for descriminators\n",
    "def binary_cross_entropy(x, z):\n",
    "    eps = 1e-12\n",
    "    return (-(x * tf.log(z + eps) + (1. - x) * tf.log(1. - z + eps)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The descriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# It takes either real or fake MNIST image 28 x 28 in grayscale\n",
    "# we use a sigmoid to make sure our output can be interpreted \n",
    "# as the probability the input image is a real MNIST character.\n",
    "def discriminator(X_in, reuse=None, keep_prob=keep_prob):\n",
    "    activation=lrelu\n",
    "    with tf.variable_scope('disc', reuse=reuse):\n",
    "        x = tf.reshape(X_in, shape=[-1, 28, 28, 1])\n",
    "        x = tf.layers.conv2d(x, kernel_size=5, filters=64, strides=2, padding='same', activation=activation)\n",
    "        x = tf.layers.dropout(x, keep_prob)\n",
    "        x = tf.layers.conv2d(x, kernel_size=5, filters=64, strides=1, padding='same', activation=activation)\n",
    "        x = tf.layers.dropout(x, keep_prob)\n",
    "        x = tf.layers.conv2d(x, kernel_size=5, filters=64, strides=1, padding='same', activation=activation)\n",
    "        x = tf.layers.dropout(x, keep_prob)\n",
    "        x = tf.layers.flatten(x)\n",
    "        x = tf.layers.dense(x, units=128, activation=activation)\n",
    "        x = tf.layers.dense(x, units=1, activation=tf.nn.sigmoid)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# z => noise\n",
    "def generator(z, keep_prob=keep_prob, is_training=is_training):\n",
    "    activation = lrelu\n",
    "    momentum = 0.99\n",
    "    with tf.variable_scope('gen', reuse=None):\n",
    "        x = z\n",
    "        d1 = 4\n",
    "        d2 = 1\n",
    "        print('checking units ', (d1 * d1 * d2))\n",
    "        x = tf.layers.dense(x, units=d1 * d1 * d2, activation=activation)\n",
    "        x = tf.layers.dropout(x, keep_prob)\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/contrib/layers/batch_norm\n",
    "        x = tf.contrib.layers.batch_norm(x, is_training=is_training, decay=momentum)\n",
    "        x = tf.reshape(x, shape=[-1, d1, d1, d2])\n",
    "        x = tf.image.resize_images(x, size=[7, 7])\n",
    "        x = tf.layers.conv2d_transpose(x, kernel_size=5, filters=64, strides=2, padding='same', activation=activation)\n",
    "        x = tf.layers.dropout(x, keep_prob)\n",
    "        x = tf.contrib.layers.batch_norm(x, is_training=is_training, decay=momentum)\n",
    "        x = tf.layers.conv2d_transpose(x, kernel_size=5, filters=64, strides=2, padding='same', activation=activation)\n",
    "        x = tf.layers.dropout(x, keep_prob)\n",
    "        x = tf.contrib.layers.batch_norm(x, is_training=is_training, decay=momentum)\n",
    "        x = tf.layers.conv2d_transpose(x, kernel_size=5, filters=64, strides=1, padding='same', activation=activation)\n",
    "        x = tf.layers.dropout(x, keep_prob)\n",
    "        x = tf.contrib.layers.batch_norm(x, is_training=is_training, decay=momentum)\n",
    "        x = tf.layers.conv2d_transpose(x, kernel_size=5, filters=64, strides=1, padding='same', activation=activation)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss functions and optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"disc/dense_2/Sigmoid:0\", shape=(?, 1), dtype=float32)\n",
      "[<tf.Variable 'gen/dense/kernel:0' shape=(200, 16) dtype=float32_ref>, <tf.Variable 'gen/dense/bias:0' shape=(16,) dtype=float32_ref>, <tf.Variable 'gen/BatchNorm/beta:0' shape=(16,) dtype=float32_ref>, <tf.Variable 'gen/conv2d_transpose/kernel:0' shape=(5, 5, 64, 1) dtype=float32_ref>, <tf.Variable 'gen/conv2d_transpose/bias:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'gen/BatchNorm_1/beta:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'gen/conv2d_transpose_1/kernel:0' shape=(5, 5, 64, 64) dtype=float32_ref>, <tf.Variable 'gen/conv2d_transpose_1/bias:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'gen/BatchNorm_2/beta:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'gen/conv2d_transpose_2/kernel:0' shape=(5, 5, 64, 64) dtype=float32_ref>, <tf.Variable 'gen/conv2d_transpose_2/bias:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'gen/BatchNorm_3/beta:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'gen/conv2d_transpose_3/kernel:0' shape=(5, 5, 1, 64) dtype=float32_ref>, <tf.Variable 'gen/conv2d_transpose_3/bias:0' shape=(1,) dtype=float32_ref>]\n",
      "[<tf.Variable 'disc/conv2d/kernel:0' shape=(5, 5, 1, 64) dtype=float32_ref>, <tf.Variable 'disc/conv2d/bias:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'disc/conv2d_1/kernel:0' shape=(5, 5, 64, 64) dtype=float32_ref>, <tf.Variable 'disc/conv2d_1/bias:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'disc/conv2d_2/kernel:0' shape=(5, 5, 64, 64) dtype=float32_ref>, <tf.Variable 'disc/conv2d_2/bias:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'disc/dense/kernel:0' shape=(12544, 128) dtype=float32_ref>, <tf.Variable 'disc/dense/bias:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'disc/dense_1/kernel:0' shape=(128, 1) dtype=float32_ref>, <tf.Variable 'disc/dense_1/bias:0' shape=(1,) dtype=float32_ref>]\n",
      "('checking d_real', TensorShape([Dimension(None), Dimension(1)]))\n",
      "('checking d_fake', TensorShape([Dimension(None), Dimension(1)]))\n",
      "(<tf.Tensor 'Neg:0' shape=(?, 1) dtype=float32>, TensorShape([Dimension(None), Dimension(1)]))\n",
      "(<tf.Tensor 'Neg_1:0' shape=(?, 1) dtype=float32>, TensorShape([Dimension(None), Dimension(1)]))\n"
     ]
    }
   ],
   "source": [
    "g = generator(noise, keep_prob, is_training)\n",
    "d_real = discriminator(X_in)\n",
    "d_fake = discriminator(g, reuse=True)\n",
    "\n",
    "print(d_real)\n",
    "vars_g = [var for var in tf.trainable_variables() if 'gen' in var.name]\n",
    "vars_d = [var for var in tf.trainable_variables() if 'disc' in var.name]\n",
    "print(vars_g)\n",
    "print(vars_d)\n",
    "\n",
    "# Applying regularizers\n",
    "d_reg = tf.contrib.layers.apply_regularization(tf.contrib.layers.l2_regularizer(1e-6), vars_d)\n",
    "g_reg = tf.contrib.layers.apply_regularization(tf.contrib.layers.l2_regularizer(1e-6), vars_g)\n",
    "\n",
    "print('checking d_real', d_real.shape)\n",
    "print('checking d_fake', d_fake.shape)\n",
    "loss_d_real = binary_cross_entropy(tf.ones_like(d_real), d_real)\n",
    "loss_d_fake = binary_cross_entropy(tf.zeros_like(d_fake), d_fake)\n",
    "loss_g = tf.reduce_mean(binary_cross_entropy(tf.ones_like(d_fake), d_fake))\n",
    "loss_d = tf.reduce_mean(0.5 * (loss_d_real + loss_d_fake))\n",
    "print(loss_d_real, loss_d_real.shape)\n",
    "print(loss_d_fake, loss_d_fake.shape)\n",
    "\n",
    "# optimizer_d = tf.train.AdamOptimizer(learning_rate).minimize(loss_d, var_list=vars_d)\n",
    "# optimizer_g = tf.train.AdamOptimizer(learning_rate).minimize(loss_g, var_list=vars_g)\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    optimizer_d = tf.train.RMSPropOptimizer(learning_rate=0.00015).minimize(loss_d + d_reg, var_list=vars_d)\n",
    "    optimizer_g = tf.train.RMSPropOptimizer(learning_rate=0.00015).minimize(loss_g + g_reg, var_list=vars_g)\n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Training GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 28, 28, 1)\n",
      "chjeckin\n",
      "checking something\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'something' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-86f15cce3651>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     )\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'checking something'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msomething\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0md_real_ls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_real_ls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0md_fake_ls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_fake_ls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'something' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    train_d = True\n",
    "    train_g = True\n",
    "    keep_prob_train = 0.6\n",
    "    \n",
    "    # Creating noise\n",
    "    n = np.random.uniform(0.0, 1.0, [batch_size, n_noise]).astype(np.float32)\n",
    "    # batch = [np.reshape(b, [28, 28]) for b in mnist.train.next_batch(batch_size=batch_size)[0]]\n",
    "    batch_x, _ = mnist.train.next_batch(batch_size)\n",
    "    batch_x = np.reshape(batch_x, newshape=[-1, 28, 28, 1])\n",
    "    print(batch_x.shape)\n",
    "    d_real_ls, d_fake_ls, g_ls, d_ls = sess.run(\n",
    "        [loss_d_real, loss_d_fake, loss_g, loss_d], feed_dict={X_in: batch_x, noise: n, keep_prob: keep_prob_train, is_training: True}\n",
    "    )\n",
    "    d_real_ls = np.mean(d_real_ls)\n",
    "    d_fake_ls = np.mean(d_fake_ls)\n",
    "    \n",
    "    g_ls = g_ls\n",
    "    d_ls = d_ls\n",
    "    \n",
    "    if g_ls * 1.5 < d_ls:\n",
    "        train_g = False\n",
    "        pass\n",
    "    \n",
    "    if d_ls * 2 < g_ls:\n",
    "        train_d = False\n",
    "        pass\n",
    "    if train_d:\n",
    "        sess.run(optimizer_d, feed_dict={noise: n, real_images: batch, keep_prob: keep_prob_train, is_training:True})\n",
    "        \n",
    "        \n",
    "    if train_g:\n",
    "        sess.run(optimizer_g, feed_dict={noise: n, keep_prob: keep_prob_train, is_training:True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
