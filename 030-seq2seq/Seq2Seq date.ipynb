{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports successful\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "from faker import Faker\n",
    "import babel\n",
    "from babel.dates import format_date\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Embedding\n",
    "\n",
    "import tensorflow.contrib.legacy_seq2seq as seq2seq\n",
    "from utilities import show_graph\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "print('Imports successful')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['en_PK', 'en_GG', 'en_TZ', 'en_VU', 'en_UG', 'en_US_POSIX', 'en_IM', 'en_GD', 'en_CC', 'en_SD', 'en_TC', 'en_NR', 'en_DM', 'en_SX', 'en_AS', 'en_PR', 'en_NU', 'en_FJ', 'en_FK', 'en_MY', 'en_SB', 'en_BE', 'en_MO', 'en_IE', 'en', 'en_BS', 'en_LR', 'en_ZW', 'en_SS', 'en_FI', 'en_MG', 'en_KE', 'en_ZA', 'en_KN', 'en_CX', 'en_TK', 'en_GB', 'en_PG', 'en_IL', 'en_DE', 'en_NA', 'en_GH', 'en_LS', 'en_IN', 'en_HK', 'en_RW', 'en_AU', 'en_PW', 'en_VI', 'en_BM', 'en_GI', 'en_NG', 'en_BB', 'en_SZ', 'en_KI', 'en_UM', 'en_ER', 'en_AI', 'en_NF', 'en_VG', 'en_CK', 'en_BI', 'en_GY', 'en_BZ', 'en_MW', 'en_DK', 'en_KY', 'en_VC', 'en_CA', 'en_WS', 'en_AG', 'en_GU', 'en_JM', 'en_IO', 'en_PH', 'en_150', 'en_NL', 'en_BW', 'en_SL', 'en_MH', 'en_DG', 'en_CY', 'en_TO', 'en_CH', 'en_TT', 'en_MT', 'en_ZM', 'en_SH', 'en_FM', 'en_SC', 'en_AT', 'en_TV', 'en_PN', 'en_SE', 'en_SI', 'en_MU', 'en_MS', 'en_CM', 'en_GM', 'en_001', 'en_MP', 'en_LC', 'en_US', 'en_SG', 'en_NZ', 'en_JE']\n"
     ]
    }
   ],
   "source": [
    "fake = Faker()\n",
    "fake.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "FORMATS = ['short',\n",
    "           'medium',\n",
    "           'long',\n",
    "           'full',\n",
    "           'd MMM YYY',\n",
    "           'd MMMM YYY',\n",
    "           'dd MMM YYY',\n",
    "           'd MMM, YYY',\n",
    "           'd MMMM, YYY',\n",
    "           'dd, MMM YYY',\n",
    "           'd MM YY',\n",
    "           'd MMMM YYY',\n",
    "           'MMMM d YYY',\n",
    "           'MMMM d, YYY',\n",
    "           'dd.MM.YY',\n",
    "           ]\n",
    "\n",
    "# change this if you want it to work with only a single language\n",
    "LOCALES = babel.localedata.locale_identifiers()\n",
    "LOCALES = [lang for lang in LOCALES if 'en' in str(lang)]\n",
    "print(LOCALES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_date():\n",
    "    \"\"\"\n",
    "        Creates some fake dates \n",
    "        :returns: tuple containing \n",
    "                  1. human formatted string\n",
    "                  2. machine formatted string\n",
    "                  3. date object.\n",
    "    \"\"\"\n",
    "    dt = fake.date_object()\n",
    "\n",
    "    # wrapping this in a try catch because\n",
    "    # the locale 'vo' and format 'full' will fail\n",
    "    try:\n",
    "        human = format_date(dt,\n",
    "                            format=random.choice(FORMATS),\n",
    "                            locale=random.choice(LOCALES))\n",
    "\n",
    "        case_change = random.randint(0,3) # 1/2 chance of case change\n",
    "        if case_change == 1:\n",
    "            human = human.upper()\n",
    "        elif case_change == 2:\n",
    "            human = human.lower()\n",
    "\n",
    "        machine = dt.isoformat()\n",
    "    except AttributeError as e:\n",
    "        return None, None, None\n",
    "\n",
    "    return human, machine #, dt\n",
    "\n",
    "data = [create_date() for _ in range(50000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'30 SEPTEMBER 2016', '2016-09-30'),\n",
       " (u'2002-03-01', '2002-03-01'),\n",
       " (u'08/12/1978', '1978-12-08'),\n",
       " (u'28 Jun 2002', '2002-06-28'),\n",
       " (u'22 Jun 1980', '1980-06-22')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('char2numX', {u'a': 0, u' ': 1, u'c': 2, u'e': 3, u'd': 4, u'h': 5, u'M': 6, u',': 7, u'1': 8, u'y': 15, u's': 10, u'r': 11, u'u': 12, u'T': 13, u'7': 14, u'9': 9, u'8': 16})\n",
      "('char2numY', {' ': 0, '-': 1, '1': 2, '0': 3, '3': 4, '2': 5, '5': 6, '4': 7, '7': 8, '6': 9, '9': 10, '8': 11})\n"
     ]
    }
   ],
   "source": [
    "# Data preprocessing\n",
    "# 1. count character occurences of each char in both X and Y\n",
    "x = [x for x, y in data]\n",
    "y = [y for x, y in data]\n",
    "\n",
    "u_characters = set(' '.join(x))\n",
    "char2numX = dict(zip(u_characters, range(len(u_characters))))\n",
    "print('char2numX', char2numX)\n",
    "\n",
    "u_characters = set(' '.join(y))\n",
    "char2numY = dict(zip(u_characters, range(len(u_characters)))) #TODO: complete\n",
    "print('char2numY', char2numY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d\n"
     ]
    }
   ],
   "source": [
    "# Pad all sequences that are shorter than the max length of the sequence\n",
    "\n",
    "char2numX['<PAD>'] = len(char2numX)\n",
    "num2charX = dict(zip(char2numX.values(), char2numX.keys()))\n",
    "max_len = max([len(date) for date in x])\n",
    "\n",
    "x = [[char2numX['<PAD>']]*(max_len - len(date)) +[char2numX[x_] for x_ in date] for date in x]\n",
    "print(''.join([num2charX[x_] for x_ in x[4]]))\n",
    "x = np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<GO>1980-06-22\n"
     ]
    }
   ],
   "source": [
    "char2numY['<GO>'] = len(char2numY)\n",
    "num2charY = dict(zip(char2numY.values(), char2numY.keys()))\n",
    "\n",
    "y = [[char2numY['<GO>']] + [char2numY[y_] for y_ in date] for date in y]\n",
    "print(''.join([num2charY[y_] for y_ in y[4]]))\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_seq_length = len(x[0])\n",
    "y_seq_length = len(y[0])- 1 # without last <END>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(x, y, batch_size):\n",
    "    shuffle = np.random.permutation(len(x))\n",
    "    start = 0\n",
    "#     from IPython.core.debugger import Tracer; Tracer()()\n",
    "    x = x[shuffle]\n",
    "    y = y[shuffle]\n",
    "    while start + batch_size <= len(x):\n",
    "        yield x[start:start+batch_size], y[start:start+batch_size]\n",
    "        start += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "batch_size = 128\n",
    "nodes = 32\n",
    "embed_size = 10\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# Tensor where we will feed the data into graph\n",
    "inputs = tf.placeholder(tf.int32, (None, x_seq_length), 'inputs')\n",
    "outputs = tf.placeholder(tf.int32, (None, None), 'output')\n",
    "targets = tf.placeholder(tf.int32, (None, None), 'targets')\n",
    "\n",
    "# Embedding layers\n",
    "input_embedding = tf.Variable(tf.random_uniform((len(char2numX), embed_size), -1.0, 1.0), name='enc_embedding')\n",
    "# TODO: create the variable output embedding\n",
    "output_embedding = tf.Variable(tf.random_uniform((len(char2numY), embed_size), -1.0, 1.0), name='dev_embedding')\n",
    "# TODO: Use tf.nn.embedding_lookup to complete the next two lines\n",
    "date_input_embed = tf.nn.embedding_lookup(input_embedding, inputs)\n",
    "date_output_embed = tf.nn.embedding_lookup(output_embedding, outputs)\n",
    "\n",
    "with tf.variable_scope(\"encoding\") as encoding_scope:\n",
    "    lstm_enc = tf.contrib.rnn.BasicLSTMCell(nodes)\n",
    "    _, last_state = tf.nn.dynamic_rnn(lstm_enc, inputs=date_input_embed, dtype=tf.float32)\n",
    "\n",
    "with tf.variable_scope(\"decoding\") as decoding_scope:\n",
    "    # TODO: create the decoder LSTMs, this is very similar to the above\n",
    "    # you will need to set initial_state=last_state from the encoder\n",
    "    lstm_dec = tf.contrib.rnn.BasicLSTMCell(nodes)\n",
    "    dec_outputs, _ = tf.nn.dynamic_rnn(lstm_dec, inputs=date_output_embed, initial_state=last_state)\n",
    "#connect outputs to \n",
    "logits = tf.contrib.layers.fully_connected(dec_outputs, num_outputs=len(char2numY), activation_fn=None) \n",
    "with tf.name_scope(\"optimization\"):\n",
    "    # Loss function\n",
    "    loss = tf.contrib.seq2seq.sequence_loss(logits, targets, tf.ones([batch_size, y_seq_length]))\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.RMSPropOptimizer(1e-3).minimize(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
